{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentimental Analysis\n",
    "This part is a combination of twitter api and google NLP api.\n",
    "The main focus is to extract tweets from twitter handle and use google NLP sevice to analyse them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## twitter part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "import yaml\n",
    "import os\n",
    "from google.oauth2 import service_account\n",
    "import googleapiclient.discovery\n",
    "from google.cloud import language_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fetch url in this block\n",
    "def create_twitter_url():\n",
    "    handle = \"MarianaSeafood\"\n",
    "    max_results = 10\n",
    "    mrf = \"max_results={}\".format(max_results)\n",
    "    q = \"query=from:{}\".format(handle)\n",
    "    url = \"https://api.twitter.com/2/tweets/search/recent?{}&{}\".format(\n",
    "        mrf, q\n",
    "    )\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch key from yaml file in this block\n",
    "def process_yaml():\n",
    "    with open(\"config.yaml\") as file:\n",
    "        return yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bearer_token_t(data):\n",
    "    return data[\"search_tweets_api\"][\"bearer_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bearer_token_g(data):\n",
    "    return [data[\"search_google_api\"][\"service_account_email\"],data[\"search_google_api\"][\"bearer_token\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_auth_and_connect(bearer_token, url):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    # print(headers)\n",
    "    response = requests.request(\"GET\", url, headers=headers)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shape the data for analysis\n",
    "def lang_data_shape(res_json):\n",
    "    data_only = res_json[\"data\"]\n",
    "    doc_start = '\"documents\": {}'.format(data_only)\n",
    "    str_json = \"{\" + doc_start + \"}\"\n",
    "    # print(str_json)\n",
    "    dump_doc = json.dumps(str_json)\n",
    "    # print(dump_doc)\n",
    "    doc = json.loads(dump_doc)\n",
    "    # print(doc)\n",
    "    return ast.literal_eval(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def create_key(service_account_email):\n",
    "    \"\"\"Creates a key for a service account.\"\"\"\n",
    "\n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "        filename=os.environ['GOOGLE_APPLICATION_CREDENTIALS'],\n",
    "        scopes=['https://www.googleapis.com/auth/cloud-platform'])\n",
    "\n",
    "    service = googleapiclient.discovery.build(\n",
    "        'iam', 'v1', credentials=credentials)\n",
    "\n",
    "    key = service.projects().serviceAccounts().keys().create(\n",
    "        name='projects/-/serviceAccounts/' + service_account_email, body={}\n",
    "        ).execute()\n",
    "\n",
    "    print('Created key: ' + key['name'])\n",
    "    return(key['name'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_analyze_sentiment(text_content):\n",
    "    \"\"\"\n",
    "    Analyzing Sentiment in a String\n",
    "\n",
    "    Args:\n",
    "      text_content The text content to analyze\n",
    "    \"\"\"\n",
    "    # client = language_v1.LanguageServiceClient()\n",
    "    client = language_v1.LanguageServiceClient.from_service_account_json(\n",
    "        \"./pro-tuner-327300-09748beb1487.json\")\n",
    "    # text_content = 'I am so happy and joyful.'\n",
    "\n",
    "    # Available types: PLAIN_TEXT, HTML\n",
    "    type_ = language_v1.Document.Type.PLAIN_TEXT\n",
    "\n",
    "    # Optional. If not specified, the language is automatically detected.\n",
    "    # For list of supported languages:\n",
    "    # https://cloud.google.com/natural-language/docs/languages\n",
    "    language = \"en\"\n",
    "    document = {\"content\": text_content, \"type_\": type_, \"language\": language}\n",
    "\n",
    "    # Available values: NONE, UTF8, UTF16, UTF32\n",
    "    encoding_type = language_v1.EncodingType.UTF8\n",
    "\n",
    "    response = client.analyze_sentiment(\n",
    "        request={'document': document, 'encoding_type': encoding_type})\n",
    "    # Get overall sentiment of the input document\n",
    "    print(u\"Document sentiment score: {}\".format(\n",
    "        response.document_sentiment.score))\n",
    "    print(\n",
    "        u\"Document sentiment magnitude: {}\".format(\n",
    "            response.document_sentiment.magnitude\n",
    "        )\n",
    "    )\n",
    "    # Get sentiment for all sentences in the document\n",
    "    for sentence in response.sentences:\n",
    "        print(u\"Sentence text: {}\".format(sentence.text.content))\n",
    "        print(u\"Sentence sentiment score: {}\".format(sentence.sentiment.score))\n",
    "        print(u\"Sentence sentiment magnitude: {}\".format(\n",
    "            sentence.sentiment.magnitude))\n",
    "\n",
    "    # Get the language of the text, which will be the same as\n",
    "    # the language specified in the request or, if not specified,\n",
    "    # the automatically-detected language.\n",
    "    print(u\"Language of the text: {}\".format(response.language))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url=https://api.twitter.com/2/tweets/search/recent?max_results=10&query=from:MarianaSeafood\n",
      "json=\n",
      "{'data': [{'id': '1447404383029841921', 'text': 'Testing twitter api'}, {'id': '1447396580504576017', 'text': 'The third test message'}], 'meta': {'newest_id': '1447404383029841921', 'oldest_id': '1447396580504576017', 'result_count': 2}}\n",
      "Text=\n",
      "Testing twitter api\n",
      "Document sentiment score: 0.10000000149011612\n",
      "Document sentiment magnitude: 0.10000000149011612\n",
      "Sentence text: Testing twitter api\n",
      "Sentence sentiment score: 0.10000000149011612\n",
      "Sentence sentiment magnitude: 0.10000000149011612\n",
      "Language of the text: en\n",
      "Text=\n",
      "The third test message\n",
      "Document sentiment score: -0.10000000149011612\n",
      "Document sentiment magnitude: 0.10000000149011612\n",
      "Sentence text: The third test message\n",
      "Sentence sentiment score: -0.10000000149011612\n",
      "Sentence sentiment magnitude: 0.10000000149011612\n",
      "Language of the text: en\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    url = create_twitter_url()\n",
    "    print(\"url=\" + url)\n",
    "\n",
    "    data = process_yaml()\n",
    "    # print(type(data))\n",
    "    # print(\"DATA=\")\n",
    "    # print(data)\n",
    "\n",
    "    bearer_token_t = create_bearer_token_t(data)\n",
    "    # print(\"Twitter TOken\")\n",
    "    # print(bearer_token_t)\n",
    "    [account_g, bearer_token_g] = create_bearer_token_g(data)\n",
    "    # print(\"Google TOken\")\n",
    "    # print(bearer_token_g)\n",
    "    # print(account_g)\n",
    "\n",
    "    res_json = twitter_auth_and_connect(bearer_token_t, url)\n",
    "    print(\"json=\")\n",
    "    print(res_json)\n",
    "\n",
    "    documents = lang_data_shape(res_json)\n",
    "    # print(\"docu=\")\n",
    "    # print(documents)\n",
    "\n",
    "    for items in documents['documents']:\n",
    "        # print(items['text'])\n",
    "        text_content = items['text']\n",
    "        print(\"Text=\")\n",
    "        print(text_content)\n",
    "        sample_analyze_sentiment(text_content)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07efdcd4b820c98a756949507a4d29d7862823915ec7477944641bea022f4f62"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
